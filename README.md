# UBA CEIA ‚Äì Trabajos Pr√°cticos de NLP

Este repositorio contiene los notebooks correspondientes a los desaf√≠os pr√°cticos del curso de **Procesamiento de Lenguaje Natural** del CEIA ‚Äì FIUBA. A continuaci√≥n, se presenta una descripci√≥n de cada notebook y sus objetivos principales.

---

## üìò Desaf√≠o 1 ‚Äì An√°lisis de Similaridad entre Documentos

**Archivo:** `Desafio_01.ipynb`

**Resumen:**  
Este trabajo implementa una estrategia para medir la similitud entre documentos utilizando la **similitud del coseno**.  
Pasos principales:

- Se seleccionan 5 documentos al azar del corpus.
- Para cada uno, se calcula la similitud con el resto usando TF-IDF o CountVectorizer.
- Se identifican los 5 documentos m√°s similares y se analizan sus etiquetas para evaluar si la similitud sem√°ntica tiene sentido.

**Objetivo:**  
Evaluar la capacidad de los modelos de vectorizaci√≥n para representar similitud sem√°ntica entre textos.

---

## üìô Desaf√≠o 2 ‚Äì Embeddings personalizados con Gensim

**Archivo:** `Desafio_02.ipynb`

**Resumen:**  
Se explora la generaci√≥n de embeddings de palabras personalizados a partir de letras de canciones en ingl√©s mediante el modelo **Word2Vec de Gensim**.

- Se entrena un modelo Word2Vec con corpus de canciones.
- Se visualizan relaciones sem√°nticas entre palabras de uso frecuente.
- Se realizan experimentos para evaluar la calidad del embedding.

**Objetivo:**  
Comprender c√≥mo el contexto espec√≠fico de un corpus afecta la representaci√≥n vectorial de las palabras.

---

## üìó Desaf√≠o 3 ‚Äì Modelo de Lenguaje con Tokenizaci√≥n por Caracteres

**Archivo:** `Desafio_3_CristianSalinas.ipynb`

**Resumen:**  
Se entrena un modelo de lenguaje basado en **RNNs, LSTM y GRU**, usando tokenizaci√≥n a nivel de car√°cter sobre letras de canciones.

- Preprocesamiento: limpieza, tokenizaci√≥n y estructuraci√≥n del dataset.
- Entrenamiento de modelos con distintas arquitecturas recurrentes.
- Generaci√≥n de texto con estrategias de Greedy Search y Beam Search (determin√≠stico y estoc√°stico).
- An√°lisis del efecto de la temperatura en la generaci√≥n de secuencias.

**Objetivo:**  
Desarrollar un modelo generativo de texto y entender el impacto de distintas arquitecturas y estrategias de inferencia.

---

## üìï Desaf√≠o 4 ‚Äì LSTM Bot QA (Conversational AI)

**Archivo:** `Desafio_4CSalinas.ipynb`

**Resumen:**  
Se construye un modelo basado en LSTM para tareas de **pregunta-respuesta (QA)** utilizando datos del challenge **ConvAI2**.

- Preprocesamiento de datos de conversaci√≥n.
- Construcci√≥n de diccionarios de vocabulario para encoder y decoder.
- Entrenamiento de un modelo encoder-decoder con embeddings y LSTM.
- Preparaci√≥n de datos para entrenar el modelo de chatbot.

**Objetivo:**  
Crear un modelo secuencia a secuencia (seq2seq) capaz de responder preguntas con una arquitectura tipo chatbot.
